\documentclass[10pt,twocolumn]{article}
\usepackage{amsmath}
\title{Scalable Video Coding Using Wavelets}
\author{Sai Karthik Vuppalapati (09d07050) \\
\and Satya Naren (09d07051) \\
\and Swrangsar Basumatary (09d07040) \\
\textbf{Group 21}}
\date{April 11, 2013}

\begin{document}

\maketitle

\begin{abstract}
    \textbf{This is a report for the Scalable Video Coding Using Wavelets application assignment in the course EE 678 Wavelets. Our goal in this assignment is to demonstrate the spatial aspect of video scalability by encoding different spatial video resolutions into a single bitstream in a scalable manner. We also compare the performance achieved by the scalable approach to the performances of single-layer approach and simulcast.}
\end{abstract}

\section{Introduction}
A scalable extension to the H.264/AVC video coding standard has been developed within the Joint Video Team (JVT), a joint organization of the ITU-T Video Coding Group (VCEG) and the ISO/IEC Moving Picture Experts Group (MPEG)\cite{segall2007}. Wavelets have the beautiful property of being inherently scalable\cite{daubechies1990, daubechies1996}. Currently there is a lot of research in the area of wavelet-based approach to scalable video coding\cite{signoroni2007}. Though the inherently scalable property of wavelets looks promising, no one has been able to come up with a wavelet-based approach to SVC that is superior in performance to the standard SVC. Here we are trying to demonstrate spatial scalability using a wavelet-based approach.

In this experiment, we decided to use \emph{three} different spatial resolutions for demonstration. At the trasmitting end, we compressed the three different resolutions and combined them into a single bitstream in a scalable way and transmitted it. At the receiver, we uncompressed the bitstream to get the resolution required.

The YUV format is known to be more efficient than RGB. We used YUV format for our video frames. Every frame has a Y, U and V component. The Y stands for the \emph{luminance} (perceptual brightness) component and U and V are the \emph{chrominance} (color) components.  The human eye is more sensitive to the luminance component Y than the chrominance components U and V. It cannot perceive the difference between an image formed by less densely sampled U and V components and the original one as long as our sampling rate is above some threshold. So we can sample the U and V components at a relatively lower rate without degrading the perceptual quality of the video. Here, the size of U and V components were chosen to be half the size of the Y component.

We have three different components Y, U and V in each frame. But from hereon, we shall treat them as one. Because we shall be performing the same operation on all the three components.

\section{Encoding the video}

At first, we reshaped every frame of the video into a $2^N * 2^N$ square frame where $N$ was chosen such that $2^N$ lies somewhere close to the greater of the two dimensions of the original frame.

Then we did the following to each frame:
\begin{enumerate}
    \item We did a 2-level 2D wavelet decomposition to get the subbands $A2$, $H2$, $V2$, $D2$, $H1$, $V1$ and $D1$.
    \item The approximation subband, $A2$, is our base layer. $H2$, $V2$ and $D2$ form our first enhancement layer. The second enhancement layer is made up of $H1$, $V1$ and $D1$.
    \item We compressed the base layer using the Set Partitioning in Hierarchial Trees (SPIHT) algorithm\cite{amirSaid1996}.
    \item We compressed the two enhancement layers by quantizing and then run length encoding them.
\end{enumerate}

\subsection*{Quantizing the subbands}

We quantized the enhancement layers to achieve a peak Signal-to-Noise ratio (PSNR) that is above a user-specified minimum value. The number of quantization levels was chosen such that the PSNR requirement is satisfied. Starting from an initial two quantization levels we kept  doubling the number of levels as long as PSNR was below a user-specified value. We double the number of levels instead of multiplying it by some other arbitrary factor because we want the quantized values to be in binary format.

\section{Transmission}

We arranged the compressed layers of each frame in the following order: the SPIHT compressed base layer, the compressed enhancement layer and then the compressed second enhancement layer.
Then we combined all the frames of the video into a single bitstream and transmitted them.

For demonstration purposes, we wrote the compressed layers into binary files and read them back at the receiver.

\section{At the receiving end}

At the receiving end, we choose our output video resolution according to the capacity of the channel. The logic used in determining the output resolution is as follows:
\begin{itemize}
    \item If the channel can accomodate the bitrate required for the largest resolution, then we go for it. We uncompress the base layer using SPIHT\cite{amirSaid1996} and run length decode the two compressed enhancement layers. And we use 2D wavelet reconstruction to get the frames of the maximum resolution video,
    \item or else if the channel can accomodate the bitrate required for the intermediate resolution, then we SPIHT uncompress the base layer and run length decode the compressed first enhancement layer. The 2D wavelet reconstruction gives the intermediate resolution video,
    \item or else if the channel can accomodate the bitrate required for the base resolution, then we uncompress the SPIHT compressed base layer to get the base resolution video,
    \item otherwise we can choose to quit or use buffering or some other techniques depending on our application.
\end{itemize}

In the experiment, we reconstructed all the three different video resolutions.


\section{Comparing scalable approach to single-layer approach and simulcast}

In single layer approach only one layer,the highest resolution layer, is SPIHT compressed and transmitted. Therefore there is less information to transmit compared to the scalable case. Thus the bitrate required is relatively low.

In simulcast all the three different resolutions are SPIHT compressed and transmitted together in a single bitstream without any scalability among the different resolutions. In simulcast there are no enhancement layers or base layers, all the three different resolutions are independent. Therefore there is more data to transmit compared to the scalable case and thus more bitrate is required than in the scalable case.

Therefore single layer approach is supposed to be faster than the scalable approach whereas the simulcast is supposed to be slowest of them all\cite{segall2007}.

To show that the speed of the scalable approach lies in between that of single layer approach and simulcast, we demonstrated the single layer approach and the simulcast also.

\subsection*{Single layer approach}
We compressed all the frames using SPIHT algorithm and transmitted them. At the receiving end, we uncompressed the frames using the same algorithm to get the output video. Only one resolution, the highest resolution is used in this approach. There are no lower or higher resolution layers.

\subsection*{Simulcast}

Here we had three layers of different resolutions for each frame. The layers were independent. They were not connected to each other in a scalable way. The layers had all the information in themselves and you need not get information from the immediate lower resolution layer. There was no concept of enhancing the immediate lower layer to get the current layer. We compressed all the three different layers of each frame using SPIHT algorithm and transmitted them.

At the receiving end, we uncompressed the compressed layers of each frame using SPIHT to get three video sequences of different resolutions.

\subsection*{Observation}
It turned out that the scalable approach required a bitrate higher than that of single-layer approach and lower than that of simulcast. In other words, the performance observed for the scalable approach is higher than that of simulcast but lower than that of the single-layer solution.

\section{Summary}

Though we have demonstrated the spatial scalability of video for only three different resolutions. This concept can be expanded to any number of resolutions. It is easier to implement dyadic scaling because 2D wavelet decomposition directly gives you a dyadically scaled down version of the original frame.

Furthermore, the performance achieved can be improved by using better compression and quantization techniques.

\bibliographystyle{plain}
\bibliography{svcReport}


\end{document}